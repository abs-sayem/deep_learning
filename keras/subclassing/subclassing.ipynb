{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Making New Layers and Models via Subclassing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The `Layer` Class: combination of state(weights) and computation**\n",
    "###### **A layer encapsulates both a state(layer's weight) and a transformation from inputs to outputs. Let's have a look a densely-connected layer. It has a state: the variables `w` and `b`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value = w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable = True\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value = b_init(shape=(units,), dtype=\"float32\"),\n",
    "            trainable = True\n",
    "        )\n",
    "    def call(self, inputs):\n",
    "        return(tf.matmul(inputs, self.w) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **We could use the layer by calling it on some tensor input(s):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04570365 -0.03962987 -0.03884789  0.09662069]\n",
      " [-0.04570365 -0.03962987 -0.03884789  0.09662069]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4, 2)     # The column size(in this case 2), must be equal to the row size of x)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **[NB] The weights `w` and `b` can be automatically tracked by the layer being set as layer attributes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **We can also add weights to a layer using the `add_weights()` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.08698852 0.05305072 0.04511479 0.02427255]\n",
      " [0.08698852 0.05305072 0.04511479 0.02427255]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "    def call(self, inputs):\n",
    "        return(tf.matmul(inputs, self.w) + self.b)\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4, 2)     # The column size(in this case 2), must be equal to the row size of x)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Layers can have Non-Trainable Weights**\n",
    "###### **Besides trainable weights, we can add non-trainable weights to a layer as well. These weights are meant not to be taken into account during backpropagation, when we are training the layer.<br>Here's how we can add and use a non-trainable weights:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return(self.total)\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **It's part of `layer.weights`, but it gets categorized as a non-trainable weight:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable weights: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"weights:\", len(my_sum.weights))\n",
    "print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))\n",
    "print(\"trainable weights:\", len(my_sum.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Unknown Input: Deferring(delay/postpone) weight creation until the shape of the inputs is known**\n",
    "###### **Our `Linear` layer above took an `input_dim` arguments that was used to compute the shape of the weights `w` and `b` in `__init__()`.<br>In many cases, we may not know in advance the size of the inputs and we would like to lazily create weights when that value becomes known, some time after instantiating the layer.<br>In the Keras API, it recommends creating layer weights in the `build(self, inputs_shape)` method of the layer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape = (input_shape[-1], self.units),      # ???\n",
    "            initializer = \"random_normal\",\n",
    "            trainable = True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (self.units,),\n",
    "            initializer = \"random_normal\",\n",
    "            trainable = True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return(tf.matmul(inputs, self.w) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **The `__call__()` method of our layer will automatically run `build()` the first time it is called. We now have a lazy layer and thus easier to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04915261 -0.01473014 -0.06646204 -0.03587055 -0.11498079  0.0668017\n",
      "   0.07341449 -0.04534969  0.23225236 -0.0580742   0.11638497  0.15088987\n",
      "   0.08120204  0.0711768   0.05008749  0.10539781  0.03635679  0.16741621\n",
      "   0.01297842 -0.1055408   0.00357105  0.06374896 -0.0660508  -0.0512604\n",
      "  -0.00384466 -0.14281851  0.01928046  0.02773319 -0.01837889 -0.03710185\n",
      "   0.06472108  0.04360595]\n",
      " [-0.04915261 -0.01473014 -0.06646204 -0.03587055 -0.11498079  0.0668017\n",
      "   0.07341449 -0.04534969  0.23225236 -0.0580742   0.11638497  0.15088987\n",
      "   0.08120204  0.0711768   0.05008749  0.10539781  0.03635679  0.16741621\n",
      "   0.01297842 -0.1055408   0.00357105  0.06374896 -0.0660508  -0.0512604\n",
      "  -0.00384466 -0.14281851  0.01928046  0.02773319 -0.01837889 -0.03710185\n",
      "   0.06472108  0.04360595]], shape=(2, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2,2))\n",
    "# At instantiation, we don't know on what inputs this is going to be called\n",
    "linear_layer = Linear(32)\n",
    "# The Layer's weights are created dynamically the first time the layer is called\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Implementing `build()` separately as shown above nicely seperates creating weights only once from using weights in every call. Layer implementers are allowed to defer weight creation to the first `__call__()`, but need to take care that, later calls use the same weights. In addition, since `__call__()` is likely to be executed for the first time inside a `tf.function`, any variable creation that takes place in `__call__()` should be wrapped in a `tf.init_scope`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Layers are Recursively Composable(Writeable)**\n",
    "###### **If we assign a layer instance as an attribute of another layer, the outer layer will start tracking the weights created by the inner layer. Keras recommend creating such subllayers in the `__init__()` method and leave it to the first `__call__()` to trigger building their weights.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable_weights: 6\n"
     ]
    }
   ],
   "source": [
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear1 = Linear(32)\n",
    "        self.linear2 = Linear(32)\n",
    "        self.linear3 = Linear(1)\n",
    "    def call(self, inputs):\n",
    "        x = self.linear1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return(self.linear3(x))\n",
    "    \n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3,64)))      # The first call to the `mlp` will create the weights\n",
    "print(\"weights:\", len(mlp.weights))\n",
    "print(\"trainable_weights:\", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The `add_loss()` Method**\n",
    "###### **While writing the call method, we can create loss tensors. That loss tensors, we will want to use later while writing our training loop. This is doable by calling `self.add_loss(value)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that creates an activity regularixation loss\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **These losses(including those created by any inner layer) can be retrived via `layer.losses`. This property is reset at the start of every `__call__()` to the top-level layer,**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24d7359adec4ffe2916680474ceb48a86338759ffb8252cd67d6683f84078a4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
