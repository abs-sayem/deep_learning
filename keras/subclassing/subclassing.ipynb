{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Making New Layers and Models via Subclassing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The `Layer` Class: combination of state(weights) and computation**\n",
    "###### **A layer encapsulates both a state(layer's weight) and a transformation from inputs to outputs. Let's have a look a densely-connected layer. It has a state: the variables `w` and `b`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value = w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable = True\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value = b_init(shape=(units,), dtype=\"float32\"),\n",
    "            trainable = True\n",
    "        )\n",
    "    def call(self, inputs):\n",
    "        return(tf.matmul(inputs, self.w) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **We could use the layer by calling it on some tensor input(s):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.01641308  0.105208   -0.06942514  0.1322065 ]\n",
      " [ 0.01641308  0.105208   -0.06942514  0.1322065 ]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4, 2)     # The column size(in this case 2), must be equal to the row size of x)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **[NB] The weights `w` and `b` can be automatically tracked by the layer being set as layer attributes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **We can also add weights to a layer using the `add_weights()` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.16506703 -0.10533164  0.02952715  0.04043421]\n",
      " [-0.16506703 -0.10533164  0.02952715  0.04043421]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_dim, units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True\n",
    "        )\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True\n",
    "        )\n",
    "    def call(self, inputs):\n",
    "        return(tf.matmul(inputs, self.w) + self.b)\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4, 2)     # The column size(in this case 2), must be equal to the row size of x)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Layers can have Non-Trainable Weights**\n",
    "###### **Besides trainable weights, we can add non-trainable weights to a layer as well. These weights are meant not to be taken into account during backpropagation, when we are training the layer.<br>Here's how we can add and use a non-trainable weights:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return(self.total)\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **It's part of `layer.weights`, but it gets categorized as a non-trainable weight:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable weights: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"weights:\", len(my_sum.weights))\n",
    "print(\"non-trainable weights:\", len(my_sum.non_trainable_weights))\n",
    "print(\"trainable weights:\", len(my_sum.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Unknown Input: Deferring(delay/postpone) weight creation until the shape of the inputs is known**\n",
    "###### **Our `Linear` layer above took an `input_dim` arguments that was used to compute the shape of the weights `w` and `b` in `__init__()`.<br>In many cases, we may not know in advance the size of the inputs and we would like to lazily create weights when that value becomes known, some time after instantiating the layer.<br>In the Keras API, it recommends creating layer weights in the `build(self, inputs_shape)` method of the layer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape = (input_shape[-1], self.units),      # ???\n",
    "            initializer = \"random_normal\",\n",
    "            trainable = True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (self.units,),\n",
    "            initializer = \"random_normal\",\n",
    "            trainable = True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return(tf.matmul(inputs, self.w) + self.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **The `__call__()` method of our layer will automatically run `build()` the first time it is called. We now have a lazy layer and thus easier to use.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.00219322  0.00839042 -0.04551635 -0.0311306   0.09423123  0.01757101\n",
      "  -0.06232182 -0.16253401  0.07216877  0.14174268 -0.13624954 -0.06687343\n",
      "  -0.15582576 -0.02411817  0.00293126 -0.16645454  0.02450774 -0.08350323\n",
      "  -0.01096236 -0.06474096  0.16049442  0.0244691  -0.07178485 -0.12176354\n",
      "   0.01975246  0.13974911 -0.05417458 -0.15414973  0.03189974 -0.02327025\n",
      "  -0.04507712  0.03664723]\n",
      " [-0.00219322  0.00839042 -0.04551635 -0.0311306   0.09423123  0.01757101\n",
      "  -0.06232182 -0.16253401  0.07216877  0.14174268 -0.13624954 -0.06687343\n",
      "  -0.15582576 -0.02411817  0.00293126 -0.16645454  0.02450774 -0.08350323\n",
      "  -0.01096236 -0.06474096  0.16049442  0.0244691  -0.07178485 -0.12176354\n",
      "   0.01975246  0.13974911 -0.05417458 -0.15414973  0.03189974 -0.02327025\n",
      "  -0.04507712  0.03664723]], shape=(2, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2,2))\n",
    "# At instantiation, we don't know on what inputs this is going to be called\n",
    "linear_layer = Linear(32)\n",
    "# The Layer's weights are created dynamically the first time the layer is called\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Implementing `build()` separately as shown above nicely seperates creating weights only once from using weights in every call. Layer implementers are allowed to defer weight creation to the first `__call__()`, but need to take care that, later calls use the same weights. In addition, since `__call__()` is likely to be executed for the first time inside a `tf.function`, any variable creation that takes place in `__call__()` should be wrapped in a `tf.init_scope`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Layers are Recursively Composable(Writeable)**\n",
    "###### **If we assign a layer instance as an attribute of another layer, the outer layer will start tracking the weights created by the inner layer. Keras recommend creating such sublayers in the `__init__()` method and leave it to the first `__call__()` to trigger building their weights.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable_weights: 6\n"
     ]
    }
   ],
   "source": [
    "class MLPBlock(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear1 = Linear(32)\n",
    "        self.linear2 = Linear(32)\n",
    "        self.linear3 = Linear(1)\n",
    "    def call(self, inputs):\n",
    "        x = self.linear1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return(self.linear3(x))\n",
    "    \n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3,64)))      # The first call to the `mlp` will create the weights\n",
    "print(\"weights:\", len(mlp.weights))\n",
    "print(\"trainable_weights:\", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The `add_loss()` Method**\n",
    "###### **While writing the call method, we can create loss tensors. That loss tensors, we will want to use later while writing our training loop. This is doable by calling `self.add_loss(value)`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that creates an activity regularixation loss\n",
    "class ActivityRegularizationLayer(keras.layers.Layer):\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **These losses(including those created by any inner layer) can be retrived via `layer.losses`. This property is reset at the start of every `__call__()` to the top-level layer, so that `layer,losses` always contains the loss values created during the last forward pass.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "    def call(self, inputs):\n",
    "        return(self.activity_reg(inputs))\n",
    "\n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0       # No losses yet since the layer has never been called\n",
    "\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1       # We created one loss value\n",
    "\n",
    "# `layer.losses` gets reset at the start of each __call__\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1       # This is the loss created during the call above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **In addition, the `loss` property also contains regularization losses created for the weights of any inner layer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: shape=(), dtype=float32, numpy=0.002138302>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayerWithKernelRegularizer(keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(OuterLayerWithKernelRegularizer, self).__init__()\n",
    "        self.dense = keras.layers.Dense(32, kernel_regularizer = tf.keras.regularizers.L2(1e-3))\n",
    "    def call(self, inputs):\n",
    "        return(self.dense(inputs))\n",
    "\n",
    "layer = OuterLayerWithKernelRegularizer()\n",
    "_ = layer(tf.zeros((1,1)))\n",
    "\n",
    "# kernel_regularizer uses this formula \"1e-3 * sum(layer.dense.kernel ** 2)\"\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **These losses are meant to be taken when writing training loops:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model = keras.Model(optimizer, loss_fn)\n",
    "\n",
    "# Iterate over the batches of a dateset\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = layer(x_batch_train)       # Logits for this minibatch\n",
    "        loss_value = loss_fn(y_batch_train, logits)     # Loss value for this minibatch\n",
    "        # Add the extra losses created during this forward pass:\n",
    "        loss_value += sum(model.losses)\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **These losses also work seemlessly with `fit()` (they get automatically summed and added to the main loss, if any):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 209ms/step - loss: 0.3589\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 0.0446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x268a182aa90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = keras.Input(shape=(3,))\n",
    "outputs = ActivityRegularizationLayer()(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# If there is a loss passed in `compile`, the regularization losses get added to it\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "model.fit(np.random.random((2,3)), np.random.random((2,3)))\n",
    "\n",
    "# It's also possible not to pass any loss in `compile`, since the model already has a loss to minimize, via the `add_loss` call during the forward pass.\n",
    "model.compile(optimizer=\"adam\")\n",
    "model.fit(np.random.random((2, 3)), np.random.random((2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The `add_metric()` Method**\n",
    "###### **Like `add_loss()`, there also has the `add_metric()` method- used for tracking the moving average of a quantity during training.<br>Consider a layer: a `logistic endpoint` layer - takes predictions and targets as input, computes the loss tracked via `add_loss()`, and then computes an accuracy scalar, which is tracks via `add_metric()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticEndpoint(keras.layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super(LogisticEndpoint, self).__init__(name=name)\n",
    "        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "        self.accuracy_fn = keras.metrics.BinaryAccuracy()\n",
    "    \n",
    "    def call(self, targets, logits, sample_weights=None):\n",
    "        # Compute the training-time loss value and add it to the layer using `self.add_loss()`\n",
    "        loss = self.loss_fn(targets, logits, sample_weights)\n",
    "        self.add_loss(loss)\n",
    "        # Copute the log accuracy as ametric and add it to the layer using `self.add_metric()`\n",
    "        acc = self.accuracy_fn(targets, logits, sample_weights)\n",
    "        self.add_metric(acc, name=\"accuracy\")\n",
    "    \n",
    "        # Return the inference-time prediction temsor (for `.prediction()`)\n",
    "        return(tf.nn.softmax(logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Metrics tracked in this way are accessible via `layer.metrics`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_metrics: [<keras.metrics.BinaryAccuracy object at 0x00000268A18258E0>]\n",
      "current_accuracy_value 1.0\n"
     ]
    }
   ],
   "source": [
    "layer = LogisticEndpoint()\n",
    "\n",
    "targets = tf.ones((2,2))\n",
    "logits = tf.ones((2,2))\n",
    "y = layer(targets, logits)\n",
    "\n",
    "print(\"layer_metrics:\", layer.metrics)\n",
    "print(\"current_accuracy_value\", float(layer.metrics[0].result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Just like `add_loss()`, these metrics are tracked by `fit()`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 444ms/step - loss: 0.9257 - binary_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x268a2b05760>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from turtle import shape\n",
    "\n",
    "\n",
    "inputs = keras.Input(shape=(3,), name=\"inputs\")\n",
    "targets = keras.Input(shape=(10,), name=\"targets\")\n",
    "logits = keras.layers.Dense(10)(inputs)\n",
    "predictions = LogisticEndpoint(name=\"predictions\")(logits, targets)\n",
    "\n",
    "model = keras.Model(inputs=[inputs, targets], outputs=predictions)\n",
    "model.compile(optimizer=\"adam\")\n",
    "\n",
    "data = {\n",
    "    \"inputs\": np.random.random((3,3)),\n",
    "    \"targets\": np.random.random((3,10)),\n",
    "}\n",
    "model.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **We can Optionally Enable Serialization on our Layers**\n",
    "###### **If we need our custom layers to be serializable as part of a `Functional Model`, we can optionally implement a `get_onfig()` method:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 32}\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = \"random_normal\",\n",
    "            trainable = True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (self.units,),\n",
    "            initializer = \"random_normal\",\n",
    "            trainable = True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return(tf.matmul(inputs, self.w) + self.b)\n",
    "\n",
    "    def get_config(self):\n",
    "        return({\"units\": self.units})\n",
    "\n",
    "# Now we can recreate the layer from its config\n",
    "layer = Linear()\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "my_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### `[NB]` **The `__init__()` method of the base `Layer` class takes some keywords arguments, in particular a `name` and a `dtypes`. It's good practice to pass these arguments to the parent class in `__Init__()` and to include them in the layer config:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_8', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape = (input_shape[-1], self.units),\n",
    "            initializer = \"random_normal\",\n",
    "            trainable = True\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (self.units,),\n",
    "            initializer = \"random_normal\",\n",
    "            trainable = True\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return(tf.matmul(inputs, self.w) + self.b)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return(config)\n",
    "\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "my_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **If we need more flexibility when deserializing the layer from its config, we can also override the `from_config()` class method. Following is the base implementation of `from_config()`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_config(cls, config):\n",
    "    return(cls(**config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Privileged `training` Argument in the `call()` Method**\n",
    "###### **Some layers, in particular the `BatchNormalization` layer and the `Dropout` layer, have different behaviors during training and inference. For such layers, it is the best practice to expose `training` (boolean) argument in the `call()` method.<br>By exposing this argument in `call()`, we enable the built-in training and evaluation loops(e.g. `fit()`) to correctly use the layer in training and inference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(keras.layers.Layer):\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        if training: return(tf.nn.dropout(inputs, rate=self.rate))\n",
    "        return(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Privileged `mask` Argument in the `call()` Method**\n",
    "###### **The other privileged argument supported by `call()` is the `mask` method.<br>A mask is a boolean tensor used to skip certain input timesteps when processing timeseries data. We will find it in all Keras RNN layers.<br>Keras will automatically pass the correct `mask` arguments to `__call__()` for layers that support it, when a mask is generated by a prior layer. Mask-generating layers are the `Embedding` layer configured with `mask_zero=True` and the `Masking` layer.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **The `Model` Class**\n",
    "###### **in general, we will use the `Layer` class to define the inner computation blocks, and will use the `Model` class to define the outer model. For instance, in a ResNet50 model, we would have several ResNet blocks subclassing `Layer`, and a single `Model` enclosing the entire ResNet network.**\n",
    "###### **The `Model` class has the same API as `Layer`, with the following differences:**\n",
    "* *It exposes built-in training, evaluation and prediction loops(`model.fit()`, `model.evaluate()`, `model.predict()`).*\n",
    "* *It exposes the list of its inner layers, via the `model.layers` property.*\n",
    "* *It exposes saving and serialization APIs(save(), save_weights()...)*\n",
    "###### **Meanwhile, the `Layer` class corresponds to what we refer to in the literature as a \"layer\"(as in `convolutional layer`, or `recurrent layer`) or as a \"block\"(as in `DNN`). And the `Model` class corresponds to what is referred to in the literature as a \"model\"(as in `deep learning model`) or as a \"network\"(as in `DNN`)**\n",
    "###### **For instance, we could take our mini-resnet example above, and use it to build a `Model` that we could train with `fit()`, and that we could save with `save_weights()`:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block1 = ResNetBlock()\n",
    "        self.block2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "    def call(self, inputs):\n",
    "        x = self.block1(inputs)\n",
    "        x = self.block2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return(self.classifier(x))\n",
    "\n",
    "resnet = ResNet()\n",
    "dataset = ...\n",
    "resnet.fit(dataset, epochs=10)\n",
    "resnet.save(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Putting all Together: an End-to-End Example**\n",
    "###### **Here's what we've learned so far:**\n",
    "* *A `Layer` encapsulate a state(created in `__init__()` or `build()`) and some computation(defined in `call()`).*\n",
    "* *Layers can be recursively nested to create new, bigger computation blocks.*\n",
    "* *Layers can create and track losses(typically regularization losses) as well as metrics, via `add_loss()` and `add_metric()`.*\n",
    "* *The outer container, the thing we want to train, is a `Model`. A `Model` is just like a `Layer`, but with added training and serialization utilities.*\n",
    "###### **Let's put all of these things together into an end-to-end example: we're going to implement a Variational AutoEncoder(VAE) and train it on MNIST utilities.<br>Our VAE will be a subclass of `Model`, built as a nested composition of layers that subclass `Layer`. It will feature a regularization loss(KL divergence).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return(z_mean + tf.exp(0.5 * z_log_var) * epsilon)\n",
    "    \n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet(z_mean, z_log_var, z).\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim=32, intermediate_dim=64, name=\"encoder\", **kwargs):\n",
    "        super(Encoder, self).__init__(name=\"name\", **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return(z_mean, z_log_var, z)\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, name=\"decoder\", **kwargs):\n",
    "        super(Decoder, self).__init__(name=\"name\", **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation=\"relu\")\n",
    "        self.dense_output = layers.Dense(original_dim, activation=\"sigmoid\")\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return(self.dense_output(x))\n",
    "    \n",
    "class VariationalAutoEncoder(keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, name=\"autoencoder\", **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=\"name\", **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss\n",
    "        kl_loss = -0.5 * tf.reduce_mean(z_log_var-tf.square(z_mean)-tf.exp(z_log_var)+1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return(reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Now, let's write a simple training loop on MNIST:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = 0.3256\n",
      "step 100: mean loss = 0.1248\n",
      "step 200: mean loss = 0.0989\n",
      "step 300: mean loss = 0.0889\n",
      "step 400: mean loss = 0.0840\n",
      "step 500: mean loss = 0.0807\n",
      "step 600: mean loss = 0.0786\n",
      "step 700: mean loss = 0.0770\n",
      "step 800: mean loss = 0.0759\n",
      "step 900: mean loss = 0.0749\n",
      "Start of epoch 1\n",
      "step 0: mean loss = 0.0746\n",
      "step 100: mean loss = 0.0739\n",
      "step 200: mean loss = 0.0734\n",
      "step 300: mean loss = 0.0730\n",
      "step 400: mean loss = 0.0727\n",
      "step 500: mean loss = 0.0722\n",
      "step 600: mean loss = 0.0720\n",
      "step 700: mean loss = 0.0717\n",
      "step 800: mean loss = 0.0714\n",
      "step 900: mean loss = 0.0712\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\")/255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "# Iterate over epochs\n",
    "for epoch in range(epochs):\n",
    "    print(\"Start of epoch %d\" % (epoch,))\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)     # Add KLD regularization loss\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "        loss_metric(loss)\n",
    "        if(step%100 == 0):\n",
    "            print(\"step %d: mean loss = %.4f\" % (step, loss_metric.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Note that, since the VAE is subclassing `Model`, it features built-in training loops. So we could also have trained it like this:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "938/938 [==============================] - 5s 4ms/step - loss: 0.0747\n",
      "Epoch 2/2\n",
      "938/938 [==============================] - 4s 4ms/step - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x159ae883460>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "vae.compile(optimizer, loss)\n",
    "vae.fit(x_train, x_train, epochs=2, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24d7359adec4ffe2916680474ceb48a86338759ffb8252cd67d6683f84078a4b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
